{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глобальные переменные и гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# путь к data\n",
    "DATA_PATH = \"../../data/task_2/\"\n",
    "# Глобальное значение \"random_state\" \n",
    "STATE = 15\n",
    "BATCH_SIZE = 2048\n",
    "VECTOR_WINDOW = 10\n",
    "VECTOR_SIZE = 300\n",
    "LEARNING_RATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"))\n",
    "print(\"Number of rows and columns in the train data set:\", train_data.shape)\n",
    "print(\"Number of rows and columns in the valid data set:\", test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['rate'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['rate'].hist(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "train_data['rate'] = le.fit_transform(train_data['rate'])\n",
    "\n",
    "# Оптимизация типов данных\n",
    "train_data['rate'] = train_data['rate'].astype('uint8')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка и предварительная обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import pymorphy2\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Загрузка русских стоп-слов\n",
    "nltk.download('stopwords')\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "# Инициализация анализатора pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Инициализируем стеммер\n",
    "stemmer = SnowballStemmer('russian')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Удаление лишних символов и нормализация\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление знаков пунктуации\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    \n",
    "    # Дополнительно: удаление специальных символов или любых символов, кроме букв (латиница или кириллица)\n",
    "    text = re.sub(r\"[^a-zA-Zа-яА-Я\\s]\", \" \", text)\n",
    "\n",
    "    # Удаление множество пробелов\n",
    "    text = text.replace(r'\\s+','')\n",
    "    \n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text, language=\"russian\")\n",
    "    \n",
    "    # Удаление стоп-слов и лемматизация\n",
    "    # words_lemmatized = [morph.parse(word)[0].normal_form for word in tokens if word not in russian_stopwords]\n",
    "    \n",
    "    # Удаление стоп-слов и стемминг\n",
    "    # words_stemmed = [stemmer.stem(word) for word in words if word not in russian_stopwords]\n",
    "    \n",
    "    return ' '.join(words_lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предварительная обработка текста\n",
    "# train_data['clear_text'] = train_data['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data['clear_text'] = test_data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.to_csv(DATA_PATH+\"train.csv\", index=False)\n",
    "# test_data.to_csv(DATA_PATH+\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop_duplicates(subset='clear_text', keep='last')\n",
    "# train_data = train_data.reset_index(drop=True)\n",
    "train_data['clear_text'] = train_data['clear_text'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Конвертация текстов в векторы с использованием обученных эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для векторизации предложений\n",
    "def vectorize_text(text, word_vectors):\n",
    "    words = text.split()\n",
    "    vectors = [word_vectors[word] for word in text if word in word_vectors]\n",
    "    # Если векторы найдены, возвращаем средний вектор, иначе вектор нулей\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word_vectors.vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Своя модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def get_word2vec_model(corpus_text: list[str], vector_size=100, window=5, min_count=1, workers=4):\n",
    "    '''\n",
    "    Ключевые параметры обучения Word2Vec:\n",
    "        sentences: Итерируемый объект (например, список), содержащий предложения, где каждое предложение представлено списком слов.\n",
    "        vector_size: Размерность векторов слов.\n",
    "        window: Максимальное расстояние между текущим и предсказываемым словом в предложении.\n",
    "        min_count: Игнорирует все слова с общей частотой ниже данного порога.\n",
    "        workers: Количество потоков для использования во время обучения.\n",
    "    '''\n",
    "    tokens_list = [word_tokenize(tokens, language=\"russian\") for tokens in corpus_text]\n",
    "    model = Word2Vec(sentences=tokens_list, vector_size=300, window=8, min_count=1, workers=4)\n",
    "    model.save(\"word2vec_model.model\")\n",
    "    return Word2Vec.load(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_word_vectors_model = get_word2vec_model(train_data['clear_text'].values,\n",
    "# \t\t\t\t\t\t\t\t  vector_size=VECTOR_SIZE,\n",
    "# \t\t\t\t\t\t\t\t  window=VECTOR_WINDOW)\n",
    "# my_word_vectors = my_word_vectors_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Получение вектора слова\n",
    "# vector = my_word_vectors['машина']  # Получение вектора для слова 'машинного'\n",
    "\n",
    "# # Поиск похожих слов\n",
    "# similar_words = my_word_vectors.most_similar('молоко')\n",
    "# print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['vector'] = train_data['clear_text'].apply(lambda text: vectorize_text(text, my_word_vectors))\n",
    "# train_data['vector'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель 65 с сайта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Путь к модели\n",
    "path_to_model = DATA_PATH + '65/model.bin'\n",
    "\n",
    "# Загрузка предварительно обученной модели (может потребовать много времени и памяти)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path_to_model, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['vector'] = train_data['clear_text'].apply(lambda text: vectorize_text(text, word_vectors))\n",
    "train_data['vector'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных для PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, vectors, labels):\n",
    "        self.vectors = vectors.reset_index(drop=True) if isinstance(vectors, pd.Series) else vectors\n",
    "        self.labels = labels.reset_index(drop=True) if isinstance(labels, pd.Series) else labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vectors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.vectors[idx], dtype=torch.float), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data['vector'].values, train_data['rate'].values, test_size=0.3, random_state=STATE, shuffle=True)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train_data['vector'].values, train_data['rate'].values, test_size=0.2, random_state=STATE, stratify=train_data['rate'])\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Тестовые данные\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]\n",
    "# train_dataset[1]\n",
    "# train_dataset[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель\n",
    "Модель TextCNN, предложенная Йун Кимом в 2014 году в статье \"Convolutional Neural Networks for Sentence Classification\", является одним из первых и наиболее влиятельных применений сверточных нейронных сетей (CNN) к задачам классификации текста. Эта модель использует операцию свертки для автоматического извлечения и обучения на важных локальных признаках предложений или документов для последующей классификации.\n",
    "\n",
    "Основные компоненты модели TextCNN:\n",
    "1. Встраивание слов (Word Embeddings):\n",
    "Модель начинается с представления каждого слова в предложении через векторы встраивания слов (word embeddings), которые могут быть инициализированы случайным образом или использовать предварительно обученные векторы, такие как Word2Vec или GloVe. Эти векторы обучаются вместе с остальной моделью для оптимизации задачи классификации.\n",
    "\n",
    "2. Слой свертки (Convolutional Layer):\n",
    "Сверточные слои применяются к векторам встраивания слов для извлечения признаков из локальных контекстов слов. В TextCNN используются несколько фильтров (ядер свертки) различных размеров (например, размером 2, 3 и 4 слова), что позволяет модели извлекать информативные признаки из различных размеров n-граммов.\n",
    "\n",
    "3. Слой пулинга (Pooling Layer):\n",
    "После сверточных слоев следует операция пулинга, обычно максимального пулинга (max-pooling), которая агрегирует информацию по всей длине предложения, выбирая наиболее важные сигналы из каждого фильтра. Это позволяет модели быть инвариантной к длине входных текстов и сосредотачиваться на наиболее значимых признаках для классификации.\n",
    "\n",
    "4. Полносвязный слой (Fully Connected Layer) и Классификация:\n",
    "Извлеченные и агрегированные признаки затем передаются через один или несколько полносвязных слоев для дополнительной обработки. Последний слой модели обычно использует функцию softmax для вычисления вероятностей принадлежности к каждому из классов. В результате модель может классифицировать входное предложение или документ в одну из предопределенных категорий.\n",
    "\n",
    "Входные параметры для класса TextCNN могут быть различными в зависимости от конкретной реализации и задачи. Ниже приведено подробное объяснение типичных входных параметров, используемых в модели TextCNN, адаптированной для классификации текста:\n",
    "\n",
    "* __pretrained_embeddings__:\n",
    "Тип: torch.Tensor\n",
    "Описание: Тензор, содержащий предварительно обученные векторные представления слов (эмбеддинги). Эти векторы используются для инициализации первого слоя модели, отвечающего за преобразование индексов слов в векторы. Параметр freeze определяет, будут ли эмбеддинги обновляться в процессе обучения.\n",
    "\n",
    "* __num_classes__:\n",
    "Тип: int\n",
    "Описание: Количество классов в задаче классификации. Этот параметр определяет размерность выходного слоя модели, поскольку выход модели должен представлять вероятности принадлежности к каждому из классов.\n",
    "\n",
    "* __filter_sizes__: \n",
    "Тип: list из int\n",
    "Описание: Список размеров фильтров (ядер) для сверточных слоев. Каждый размер фильтра соответствует \"ширине\" окна, через которое модель \"смотрит\" на входные данные, позволяя извлекать признаки из n-грамм различной длины. Например, filter_sizes=[3, 4, 5] означает, что будут использоваться три типа фильтров, обрабатывающие 3, 4 и 5 слов одновременно.\n",
    "\n",
    "* __num_filters__:\n",
    "Тип: int\n",
    "Описание: Количество фильтров (или \"каналов\" в терминах сверточных сетей) для каждого размера фильтра. Этот параметр определяет \"глубину\" выучиваемых признаков: больше фильтров может позволить модели лучше абстрагироваться и извлекать более сложные признаки, но также увеличивает количество параметров модели и потребность в вычислительных ресурсах.\n",
    "\n",
    "* __dropout__:\n",
    "Тип: float\n",
    "Описание: Параметр, контролирующий долю нейронов, которые случайным образом \"отключаются\" во время обучения для предотвращения переобучения. Значение 0.5 означает, что каждый нейрон имеет 50% шанс быть исключенным при каждом проходе вперед по сети во время обучения. Dropout помогает улучшить обобщающую способность модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_embeddings, \n",
    "                 num_classes, \n",
    "                 filter_sizes=[3, 4, 5], \n",
    "                 num_filters=100,\n",
    "                 dropout=0.8):\n",
    "        \n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, pretrained_embeddings.size(1))) for k in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # [Batch, Sequence Length, Embedding Dim]\n",
    "        x = x.unsqueeze(1)  # [Batch, 1, Sequence Length, Embedding Dim] для Conv2d\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # Применяем свертку и активацию\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # Максимальный пулинг\n",
    "        x = torch.cat(x, 1)  # Конкатенация признаков из всех фильтров\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы адаптировать модель TextCNN под использование предварительно обученных векторов, предположим, что каждый \"документ\" или \"текст\" уже представлен усредненным вектором (или любым другим методом агрегации эмбеддингов). Модель должна быть способна принимать эти векторы напрямую без слоя эмбеддинга:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdaptedTextCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim,  # Размерность предобученных эмбеддингов\n",
    "                 num_classes, \n",
    "                 dropout=0.1):\n",
    "        \n",
    "        super(AdaptedTextCNN, self).__init__()\n",
    "        \n",
    "        # Полносвязные слои для классификации агрегированных эмбеддингов\n",
    "        self.fc1 = nn.Linear(embedding_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, embedding_dim]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренер\n",
    "\n",
    "Класс Trainer был разработан для организации и упрощения процесса обучения моделей глубокого обучения в PyTorch. Он инкапсулирует логику обучения, валидации и тестирования, а также визуализацию результатов в единый интерфейс. Это позволяет избежать дублирования кода и делает эксперименты более структурированными и воспроизводимыми. Давайте подробно рассмотрим аргументы конструктора класса Trainer и их роли.\n",
    "\n",
    "## Аргументы класса Trainer:\n",
    "__model__:\n",
    "Тип: torch.nn.Module\n",
    "Описание: Это нейронная сеть, которую вы хотите обучить. Модель должна быть подклассом torch.nn.Module. Экземпляр модели будет использоваться для выполнения прямого распространения, обратного распространения и обновления весов.\n",
    "\n",
    "__train_loader__:\n",
    "Тип: torch.utils.data.DataLoader\n",
    "Описание: DataLoader, который предоставляет доступ к обучающему набору данных. Он используется для итерации по обучающему датасету во время обучения модели. DataLoader обеспечивает эффективное извлечение данных с поддержкой многопоточности и батчинга.\n",
    "\n",
    "__test_loader__:\n",
    "Тип: torch.utils.data.DataLoader\n",
    "Описание: Подобно train_loader, но предоставляет доступ к тестовому набору данных. Используется для оценки производительности модели на отдельном тестовом датасете после обучения.\n",
    "\n",
    "__criterion__ (необязательный):\n",
    "Тип: torch.nn.modules.loss._Loss\n",
    "Описание: Функция потерь, используемая для оценки производительности модели и ее обучения. Примеры включают nn.CrossEntropyLoss для задач классификации или nn.MSELoss для регрессии. Если не указано, должно быть предоставлено значение по умолчанию внутри класса.\n",
    "\n",
    "__optimizer__ (необязательный):\n",
    "Тип: torch.optim.Optimizer\n",
    "Описание: Оптимизатор, используемый для обновления весов модели в процессе обучения. Например, optim.Adam или optim.SGD. Как и в случае с criterion, если не указано, должно быть установлено значение по умолчанию.\n",
    "\n",
    "__device__ (необязательный):\n",
    "Тип: torch.device\n",
    "Описание: Устройство, на котором должны выполняться вычисления (CPU или GPU). Это позволяет явно контролировать, где будут выполняться операции с тензорами и моделью. Если не указано, можно автоматически определить доступное устройство."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, \n",
    "                 model: AdaptedTextCNN, \n",
    "                 train_loader, \n",
    "                 test_loader, \n",
    "                 criterion=None, \n",
    "                 optimizer=None, \n",
    "                 device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.criterion = criterion if criterion else nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimizer if optimizer else optim.Adam(model.parameters())\n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "        self.test_f1_scores = []\n",
    "\n",
    "    # Обучение\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        for vectors, labels in self.train_loader:\n",
    "            vectors, labels = vectors.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # print(self.model.state_dict())\n",
    "   \n",
    "      \n",
    "            outputs = self.model(vectors)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "   \n",
    "            # print(self.model.state_dict())\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        self.train_losses.append(avg_loss)\n",
    "        # print(f'Training Loss: {avg_loss:.4f}')\n",
    "\n",
    "    def test_epoch(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for vectors, labels in self.test_loader:\n",
    "                vectors, labels = vectors.to(self.device), labels.to(self.device)               \n",
    "    \n",
    "                outputs = self.model(vectors)\n",
    "                outputs = F.softmax(outputs, dim=1)  # Применяем softmax\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "                # outputs = self.model(vectors)    \n",
    "                # loss = self.criterion(outputs, labels)\n",
    "                # running_loss += loss.item()                \n",
    "                # _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_loss = running_loss / len(self.test_loader)\n",
    "        accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        self.test_losses.append(avg_loss)\n",
    "        self.test_accuracies.append(accuracy)\n",
    "        self.test_f1_scores.append(f1)\n",
    "        # print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch+1}/{epochs}')\n",
    "            self.train_epoch()\n",
    "            self.test_epoch()\n",
    "            print(f'Training Loss: {self.train_losses[-1]:.4f}, Test Loss: {self.test_losses[-1]:.4f}, Accuracy: {self.test_accuracies[-1]:.4f}, F1 Score: {self.test_f1_scores[-1]:.4f}')\n",
    "\n",
    "    \n",
    "    def plot_training(self):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.test_losses, label='Test Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss over Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(self.test_accuracies, label='Test Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy over Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(self.test_f1_scores, label='Test F1 Score')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('F1 Score over Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение\n",
    "\n",
    "Чтобы использовать эмбеддинги, полученные с помощью модели Word2Vec, в обучении модели TextCNN на PyTorch, вам необходимо выполнить несколько шагов:\n",
    "\n",
    "1. Извлечение матрицы весов эмбеддингов из обученной модели Word2Vec.\n",
    "2. Создание слоя эмбеддингов в модели TextCNN с использованием извлеченной матрицы весов.\n",
    "3. Адаптация данных под формат, принимаемый моделью TextCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device - \", device)\n",
    "# # Получение матрицы весов эмбеддингов\n",
    "# weights = torch.FloatTensor(my_word_vectors.vectors)\n",
    "# vocab_size, embedding_dim = weights.size()\n",
    "\n",
    "weights = torch.FloatTensor(word_vectors.vectors)\n",
    "vocab_size, embedding_dim = weights.size()\n",
    "\n",
    "# Создание экземпляра модели\n",
    "\n",
    "# model = TextCNN(pretrained_embeddings=weights,\n",
    "#                 num_classes=5,\n",
    "#                 filter_sizes=[3, 4, 5], \n",
    "# \t\t\t\tnum_filters=100,\n",
    "# \t\t\t\tdropout=0.5).to(device)\n",
    "\n",
    "model =  AdaptedTextCNN(embedding_dim=embedding_dim,\n",
    "                num_classes=5,\n",
    "                dropout=0.01).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Создание экземпляра Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "\n",
    "# Запуск процесса обучения\n",
    "trainer.fit(epochs=100)\n",
    "\n",
    "# Визуализация процесса обучения\n",
    "trainer.plot_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(epochs=100)\n",
    "trainer.plot_training()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предикт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формирование решения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict_proba(test_data['clear_text'])\n",
    "# Get classes\n",
    "predictions = np.argmax(y_pred, axis=1)\n",
    "# pred_labels = le.inverse_transform(preds)\n",
    "\n",
    "sample_submission = pd.read_csv(os.path.join(DATA_PATH, \"sample_submission.csv\"))\n",
    "sample_submission[\"rate\"] = predictions\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission.to_csv(DATA_PATH+\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
