{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# путь к data\n",
    "DATA_PATH = \"../../data/task_2/\"\n",
    "# Глобальное значение \"random_state\" \n",
    "STATE = 42\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"))\n",
    "\n",
    "print(\"Number of rows and columns in the train data set:\", train_data.shape)\n",
    "print(\"Number of rows and columns in the valid data set:\", test_data.shape)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['rate'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['rate'].hist(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "train_data['rate'] = le.fit_transform(train_data['rate'])\n",
    "\n",
    "# Оптимизация типов данных\n",
    "train_data['rate'] = train_data['rate'].astype('uint8')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import pymorphy2\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Загрузка русских стоп-слов\n",
    "nltk.download('stopwords')\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "\n",
    "# Инициализация анализатора pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Инициализируем стеммер\n",
    "stemmer = SnowballStemmer('russian')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Удаление лишних символов и нормализация\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление знаков пунктуации\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    \n",
    "    # Дополнительно: удаление специальных символов или любых символов, кроме букв (латиница или кириллица)\n",
    "    text = re.sub(r\"[^a-zA-Zа-яА-Я\\s]\", \" \", text)\n",
    "\n",
    "    # Удаление множество пробелов\n",
    "    text = text.replace(r'\\s+','')\n",
    "    \n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text, language=\"russian\")\n",
    "    \n",
    "    # Удаление стоп-слов и лемматизация\n",
    "    words_lemmatized = [morph.parse(word)[0].normal_form for word in tokens if word not in russian_stopwords]\n",
    "    \n",
    "    # Удаление стоп-слов и стемминг\n",
    "    # words_stemmed = [stemmer.stem(word) for word in words if word not in russian_stopwords]\n",
    "    \n",
    "    return ' '.join(words_lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['clear_text'] = train_data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data['clear_text'] = train_data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class FiveDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_seq_len):\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe['text'].tolist()\n",
    "        self.targets = None\n",
    "        if 'rate' in dataframe:\n",
    "            self.targets = dataframe['rate'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_seq_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        if self.targets is not None:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "class ModelForClassification(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model_path: str, config: Dict):\n",
    "        super(ModelForClassification, self).__init__()\n",
    "        self.model_name = model_path\n",
    "        self.config = config\n",
    "        self.n_classes = config['num_classes']\n",
    "        self.dropout_rate = config['dropout_rate']\n",
    "        self.bert = AutoModel.from_pretrained(self.model_name)\n",
    "        self.pre_classifier = torch.nn.Linear(312, 768)\n",
    "        self.dropout = torch.nn.Dropout(self.dropout_rate)\n",
    "        self.classifier = torch.nn.Linear(768, self.n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,):\n",
    "        output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        hidden_state = output[0]\n",
    "        hidden_state = hidden_state[:, 0]\n",
    "        hidden_state = self.pre_classifier(hidden_state)\n",
    "        hidden_state = torch.nn.ReLU()(hidden_state)\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        output = self.classifier(hidden_state)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Тренер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from numpy import asarray\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.n_epochs = config['n_epochs']\n",
    "        self.optimizer = None\n",
    "        self.opt_fn = lambda model: Adam(model.parameters(), config['lr'])\n",
    "        self.model: ModelForClassification = None\n",
    "        self.history = None\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        self.device = config['device']\n",
    "        self.verbose = config.get('verbose', True)\n",
    "\n",
    "    def fit(self, model, train_dataloader, val_dataloader):\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = self.opt_fn(model)\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{self.n_epochs}\")\n",
    "            train_info = self.train_epoch(train_dataloader)\n",
    "            val_info = self.val_epoch(val_dataloader)\n",
    "            self.history['train_loss'].extend(train_info['loss'])\n",
    "            self.history['val_loss'].extend([val_info['loss']])\n",
    "            self.history['val_acc'].extend([val_info['acc']])\n",
    "        return self.model.eval()\n",
    "\n",
    "    def train_epoch(self, train_dataloader):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "        if self.verbose:\n",
    "            train_dataloader = tqdm(train_dataloader)\n",
    "        for batch in train_dataloader:\n",
    "            ids = batch['ids'].to(self.device, dtype=torch.long)\n",
    "            mask = batch['mask'].to(self.device, dtype=torch.long)\n",
    "            targets = batch['targets'].to(self.device, dtype=torch.long)\n",
    "\n",
    "            outputs = self.model(ids, mask)\n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_val = loss.item()\n",
    "            if self.verbose:\n",
    "                train_dataloader.set_description(f\"Loss={loss_val:.3}\")\n",
    "            losses.append(loss_val)\n",
    "        return {'loss': losses}\n",
    "\n",
    "    def val_epoch(self, val_dataloader):\n",
    "        self.model.eval()\n",
    "        all_logits = []\n",
    "        all_labels = []\n",
    "        if self.verbose:\n",
    "            val_dataloader = tqdm(val_dataloader)\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                ids = batch['ids'].to(self.device, dtype=torch.long)\n",
    "                mask = batch['mask'].to(self.device, dtype=torch.long)\n",
    "                targets = batch['targets'].to(self.device, dtype=torch.long)\n",
    "                outputs = self.model(ids, mask)\n",
    "                all_logits.append(outputs)\n",
    "                all_labels.append(targets)\n",
    "        all_labels = torch.cat(all_labels).to(self.device)\n",
    "        all_logits = torch.cat(all_logits).to(self.device)\n",
    "        loss = self.loss_fn(all_logits, all_labels).item()\n",
    "        acc = (all_logits.argmax(1) == all_labels).float().mean().item()\n",
    "        print(acc)\n",
    "        if self.verbose:\n",
    "            val_dataloader.set_description(f\"Loss={loss:.3}; Acc:{acc:.3}\")\n",
    "        return {\n",
    "            'acc': acc,\n",
    "            'loss': loss\n",
    "        }\n",
    "\n",
    "    def predict(self, test_dataloader):\n",
    "        if not self.model:\n",
    "            raise RuntimeError(\"You should train the model first\")\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                ids = batch['ids'].to(self.device, dtype=torch.long)\n",
    "                mask = batch['mask'].to(self.device, dtype=torch.long)\n",
    "                outputs = self.model(ids, mask)\n",
    "                predictions.extend(outputs.argmax(1).tolist())\n",
    "        return asarray(predictions)\n",
    "\n",
    "    def save(self, path: str):\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"You should train the model first\")\n",
    "        checkpoint = {\n",
    "            \"config\": self.model.config,\n",
    "            \"trainer_config\": self.config,\n",
    "            \"model_name\": self.model.model_name,\n",
    "            \"model_state_dict\": self.model.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        ckpt = torch.load(path)\n",
    "        keys = [\"config\", \"trainer_config\", \"model_state_dict\"]\n",
    "        for key in keys:\n",
    "            if key not in ckpt:\n",
    "                raise RuntimeError(f\"Missing key {key} in checkpoint\")\n",
    "        new_model = ModelForClassification(\n",
    "            ckpt['model_name'],\n",
    "            ckpt[\"config\"]\n",
    "        )\n",
    "        new_model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "        new_trainer = cls(ckpt[\"trainer_config\"])\n",
    "        new_trainer.model = new_model\n",
    "        new_trainer.model.to(new_trainer.device)\n",
    "        return new_trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выполнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_small = train_data[['text', 'rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, val_split = train_test_split(train_data, test_size=0.2, random_state=42, stratify=train_data['rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"cointegrated/rubert-tiny2\", truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FiveDataset(train_split, tokenizer, MAX_LEN)\n",
    "val_dataset = FiveDataset(val_split, tokenizer, MAX_LEN)\n",
    "test_dataset = FiveDataset(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\"batch_size\": BATCH_SIZE,\n",
    "                \"shuffle\": True,\n",
    "                \"num_workers\": 0\n",
    "                }\n",
    "\n",
    "test_params = {\"batch_size\": BATCH_SIZE,\n",
    "               \"shuffle\": False,\n",
    "               \"num_workers\": 0\n",
    "               }\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, **train_params)\n",
    "val_dataloader = DataLoader(val_dataset, **test_params)\n",
    "test_dataloader = DataLoader(test_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_classes\": 5,\n",
    "    \"dropout_rate\": 0.1\n",
    "}\n",
    "model = ModelForClassification(\n",
    "    \"cointegrated/rubert-tiny2\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"n_epochs\": 5,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\": 42,\n",
    "}\n",
    "t = Trainer(trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(DATA_PATH+\"baseline_model.ckpt\")\n",
    "t = Trainer.load(DATA_PATH+\"baseline_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = t.predict(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(os.path.join(DATA_PATH, \"sample_submission.csv\"))\n",
    "pred_labels = le.inverse_transform(predictions)\n",
    "sample_submission[\"rate\"] = predictions\n",
    "sample_submission['rate'] = le.inverse_transform(sample_submission['rate'])\n",
    "\n",
    "# sample_submission.loc[sample_submission['rate'] == 5, 'rate'] = 4\n",
    "sample_submission['rate'] = sample_submission['rate'].apply(lambda a: a + 1)\n",
    "\n",
    "sample_submission['rate'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(DATA_PATH+\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
