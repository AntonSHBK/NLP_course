{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean(inp: str) -> str:\n",
    "\tinp = inp.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
    "\tinp = re.sub(r'\\s+', ' ', inp.lower())\n",
    "\treturn inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'empathy for the poor may not come easily to people who never experienced it they may blame the victims and insist their predicament can be overcome through determination and hard work'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/text.txt', 'r') as f:\n",
    "\ttest_text = f.readlines()\n",
    "\ttest_text = ' '.join(test_text)\n",
    "\n",
    "test_text = \"\"\"Empathy for the poor may not come easily to people who never experienced it. They may blame the victims and insist their predicament can be overcome through determination and hard work\"\"\"\n",
    "clear_test_text = clean(test_text)\n",
    "clear_test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Epoch 1, Loss: 1169.1363234519958\n",
      "Epoch 2, Loss: 1101.0393064022064\n",
      "Epoch 3, Loss: 1057.7352521419525\n",
      "Epoch 4, Loss: 1027.1564388275146\n",
      "Epoch 5, Loss: 1003.3897905349731\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Функция для создания словаря и подготовки данных\n",
    "def prepare_data_skip_gram(text: str, window_size=2):\n",
    "\t# Удаляем все символы кроме a-z, @, и #\n",
    "\ttext = re.sub(r'[^a-z@# ]', '', text.lower())    \n",
    "\t# Разбиваем на пробелы\n",
    "\ttokens = text.split()    \n",
    "\t\n",
    "\tvocab = set(tokens)\n",
    "\tword_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\t\n",
    "\tdata = []\n",
    "\tfor i in range(len(tokens)):\n",
    "\t\tfor j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n",
    "\t\t\tif i != j:\n",
    "\t\t\t\tdata.append((tokens[i], tokens[j]))\t\n",
    "\treturn data, word_to_ix, len(vocab)\t\n",
    " \n",
    "class SkipGramDataset(Dataset):\n",
    "\tdef __init__(self, data, word_to_ix):\t\t\t\n",
    "\t\tself.data = [(word_to_ix[center], word_to_ix[context]) for center, context in data]\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn torch.tensor(self.data[idx][0], dtype=torch.long), torch.tensor(self.data[idx][1], dtype=torch.long)\n",
    "\t\n",
    "\n",
    "class Word2VecSkipGramModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embedding_dim):\n",
    "\t\tsuper(Word2VecSkipGramModel, self).__init__()\n",
    "\t\tself.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\t\tself.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\t\tself.activation_function = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "\tdef forward(self, center_word_idx):\n",
    "\t\thidden_layer = self.embeddings(center_word_idx)\n",
    "\t\tout_layer = self.out_layer(hidden_layer)\n",
    "\t\tlog_probs = self.activation_function(out_layer)\n",
    "\t\treturn log_probs\n",
    "\n",
    "# Функция обучения модели\n",
    "def train_model(data, word_to_ix, vocab_size, embedding_dim=50, epochs=10, batch_size=1):\n",
    "\tdataset = SkipGramDataset(data, word_to_ix)\n",
    "\tdataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\t\n",
    "\tmodel = Word2VecSkipGramModel(vocab_size, embedding_dim)\n",
    "\tloss_function = nn.NLLLoss()\n",
    "\toptimizer = torch.optim.SGD (model.parameters(), lr=0.05)\n",
    "\tprint('start')\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor center_word, context_word in dataloader:\n",
    "\t\t\tmodel.zero_grad()\n",
    "\t\t\tlog_probs = model(center_word)\n",
    "\t\t\tloss = loss_function(log_probs, context_word)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()            \n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\t\t\n",
    "\t\tprint(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "\treturn model\n",
    "\n",
    "# Главная функция\n",
    "def train(data: str):\n",
    "\twindow_size = 2\n",
    "\tembedding_dim = 10\n",
    "\tepochs = 5\n",
    "\tbatch_size = 2\n",
    "\t\n",
    "\tngramm_data, word_to_ix, vocab_size = prepare_data_skip_gram(data, window_size)    \n",
    "\tmodel = train_model(ngramm_data, word_to_ix, vocab_size, embedding_dim, epochs, batch_size)\n",
    "\t\n",
    "\t# # Извлекаем векторы слов из модели\n",
    "\tembeddings = model.embeddings.weight.data.numpy()\n",
    "\tix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "\tw2v_dict = {ix_to_word[ix]: embeddings[ix] for ix in range(vocab_size)}\n",
    "\treturn w2v_dict\n",
    "# Тестовые данные\n",
    "test_text = 'Captures Semantic Relationships: The skip-gram model effectively captures semantic relationships between words. It learns word embeddings that encode similar meanings and associations, allowing for tasks like word analogies and similarity calculations. Handles Rare Words: The skip-gram model performs well even with rare words or words with limited occurrences in the training data. It can generate meaningful representations for such words by leveraging the context in which they appear. Contextual Flexibility: The skip-gram model allows for flexible context definitions by using a window around each target word. This flexibility captures local and global word associations, resulting in richer semantic representations. Scalability: The skip-gram model can be trained efficiently on large-scale datasets due to its simplicity and parallelization potential. It can process vast amounts of text data to generate high-quality word embeddings.'\n",
    "\n",
    "w2v_dict = train(test_text)\n",
    "# print(w2v_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 283.4514169692993\n",
      "Epoch 2, Loss: 283.341778755188\n",
      "Epoch 3, Loss: 283.232702255249\n",
      "Epoch 4, Loss: 283.1228437423706\n",
      "Epoch 5, Loss: 283.01355934143066\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def prepare_data_cbow(text: str, window_size=2):\n",
    "\ttext = re.sub(r'[^a-z@# ]', '', text.lower())    \n",
    "\ttokens = text.split()    \n",
    "\t\n",
    "\tvocab = set(tokens)\n",
    "\tword_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\t\n",
    "\tdata = []\n",
    "\tfor i in range(window_size, len(tokens) - window_size):\n",
    "\t\tcontext = [tokens[i - j - 1] for j in range(window_size)] + [tokens[i + j + 1] for j in range(window_size)]\n",
    "\t\ttarget = tokens[i]\n",
    "\t\tdata.append((context, target))\n",
    "\treturn data, word_to_ix, len(vocab)\t\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "\tdef __init__(self, data, word_to_ix):\n",
    "\t\tself.contexts = []\n",
    "\t\tself.targets = []\n",
    "\t\tfor context, target in data:\n",
    "\t\t\tindexed_context = [word_to_ix[word] for word in context]\n",
    "\t\t\tself.contexts.append(indexed_context)\n",
    "\t\t\tself.targets.append(word_to_ix[target])\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.targets)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t# Возвращаем контекст и центральное слово как пару тензоров\n",
    "\t\treturn torch.tensor(self.contexts[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "class Word2VecCBOWModel(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embedding_dim):\n",
    "\t\tsuper(Word2VecCBOWModel, self).__init__()\n",
    "\t\tself.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\t\tself.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\t\tself.activation_function = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\tdef forward(self, center_word_idx):\n",
    "\t\thidden_layer = torch.mean(self.embeddings(center_word_idx), dim=1)\n",
    "\t\tout_layer = self.out_layer(hidden_layer)\n",
    "\t\tlog_probs = self.activation_function(out_layer)\n",
    "\t\treturn log_probs\n",
    "\n",
    "# Функция обучения модели\n",
    "def train_model_cbow(data, word_to_ix, vocab_size, embedding_dim=50, epochs=10, batch_size=1):\n",
    "\tdataset = CBOWDataset(data, word_to_ix)\n",
    "\tdataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\t\n",
    "\tmodel = Word2VecCBOWModel(vocab_size, embedding_dim)\n",
    "\tloss_function = nn.NLLLoss()\n",
    "\toptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\t\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor context_words, target_word in dataloader:\n",
    "\t\t\tcontext_words = context_words  # Подготавливаем контекстные слова\n",
    "\t\t\tmodel.zero_grad()\n",
    "\t\t\tlog_probs = model(context_words)\n",
    "\t\t\tloss = loss_function(log_probs, target_word)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttotal_loss += loss.item()\n",
    "\t\tprint(f'Epoch {epoch + 1}, Loss: {total_loss}')\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# Главная функция\n",
    "def train(data: str):\n",
    "\twindow_size = 2\n",
    "\tembedding_dim = 10\n",
    "\tepochs = 5\n",
    "\tbatch_size = 2\n",
    "\t\n",
    "\tngramm_data, word_to_ix, vocab_size = prepare_data_cbow(data, window_size)    \n",
    "\tmodel = train_model_cbow(ngramm_data, word_to_ix, vocab_size, embedding_dim, epochs, batch_size)\n",
    "\t\n",
    "\t# # Извлекаем векторы слов из модели\n",
    "\tembeddings = model.embeddings.weight.data.numpy()\n",
    "\tix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "\tw2v_dict = {ix_to_word[ix]: embeddings[ix] for ix in range(vocab_size)}\n",
    "\treturn w2v_dict\n",
    "# Тестовые данные\n",
    "test_text = 'Captures Semantic Relationships: The skip-gram model effectively captures semantic relationships between words. It learns word embeddings that encode similar meanings and associations, allowing for tasks like word analogies and similarity calculations. Handles Rare Words: The skip-gram model performs well even with rare words or words with limited occurrences in the training data. It can generate meaningful representations for such words by leveraging the context in which they appear. Contextual Flexibility: The skip-gram model allows for flexible context definitions by using a window around each target word. This flexibility captures local and global word associations, resulting in richer semantic representations. Scalability: The skip-gram model can be trained efficiently on large-scale datasets due to its simplicity and parallelization potential. It can process vast amounts of text data to generate high-quality word embeddings.'\n",
    "\n",
    "w2v_dict = train(test_text)\n",
    "# print(w2v_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
