{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = Path('../docs/imgs/')\n",
    "DATA_PATH = Path('../data/project/')\n",
    "\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)\n",
    "\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path: Path) -> pd.DataFrame:\n",
    "    data = pd.read_excel(file_path)    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = load_data(os.path.join(DATA_PATH, \"data_1.xlsx\"))\n",
    "print(\"Number of rows and columns in the train data set:\", dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(dataframe: pd.DataFrame, column: str):\n",
    "    category_counts = dataframe.groupby(column).size().sort_values(ascending=False)\n",
    "\n",
    "    # Calculating percentages\n",
    "    total_messages = category_counts.sum()\n",
    "    percentages = (category_counts / total_messages * 100).round(2)\n",
    "\n",
    "    # Plotting a bar chart for the message counts by category\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.bar(category_counts.index, category_counts.values, color='skyblue')\n",
    "\n",
    "    plt.title('Распределение сообщений по '+column, fontsize=15, fontweight='bold')\n",
    "    plt.xlabel(column, fontsize=12)\n",
    "    plt.ylabel('Количество сообщений', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    \n",
    "    # Adding absolute values and percentages on top of each bar\n",
    "    for bar, percentage in zip(bars, percentages):\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, f'{int(yval)}\\n({percentage}%)', ha='center', va='bottom', fontsize=7)\n",
    "        \n",
    "\n",
    "    plt.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    save_fig(column+'_hist')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(dataframe, 'type_problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(dataframe, 'categoria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Создаем объект Digraph для генерации диаграммы\n",
    "dot = Digraph(comment='Custom GPT-2 Model Architecture')\n",
    "\n",
    "# Добавляем узлы, соответствующие компонентам модели\n",
    "dot.node('Input', 'Input IDs')\n",
    "dot.node('Emb', 'Token Embeddings (wte)')\n",
    "dot.node('TypeEmb', 'Type Embeddings')\n",
    "dot.node('Combine', 'Combined Embeddings')\n",
    "dot.node('Lin', 'Linear Layer (combined_linear)')\n",
    "dot.node('GPT2', 'GPT-2 Processing')\n",
    "dot.node('Head', 'Output Head (lm_head)')\n",
    "dot.node('Logits', 'Logits')\n",
    "\n",
    "# Создаем связи между узлами для отображения потока данных\n",
    "dot.edge('Input', 'Emb', label='token ids')\n",
    "dot.edge('Input', 'TypeEmb', label='type ids')\n",
    "dot.edge('Emb', 'Combine', label='embeddings')\n",
    "dot.edge('TypeEmb', 'Combine', label='type embeddings')\n",
    "dot.edge('Combine', 'Lin', label='combined embeddings')\n",
    "dot.edge('Lin', 'GPT2', label='input to GPT-2')\n",
    "dot.edge('GPT2', 'Head', label='last hidden state')\n",
    "dot.edge('Head', 'Logits', label='output logits')\n",
    "\n",
    "# Визуализируем график\n",
    "dot.render(IMAGES_PATH/'CustomGPT2ModelArchitecture', format='png', view=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.dropna(subset=['target', 'source'])\n",
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "work_dataframe = dataframe.copy()\n",
    "\n",
    "class CategoricalLabelEncoder():\n",
    "    def __init__(self, dataframe: pd.DataFrame, categorical_columns: list[str]) -> None:\n",
    "        self.encoders = {}\n",
    "        \n",
    "        for column in categorical_columns:\n",
    "            encoder = LabelEncoder()\n",
    "            dataframe[column] = encoder.fit_transform(dataframe[column])\n",
    "            self.encoders[column] = encoder\n",
    "        \n",
    "    def decode(self, label: str, code: list[int]):\n",
    "        return self.encoders[label].inverse_transform([code])\n",
    "    \n",
    "    def get_classes(self, label):\n",
    "        categories_list = self.encoders[label].classes_\n",
    "        print(\"Список категорий в 'categoria':\", categories_list)\n",
    "        return categories_list\n",
    "\n",
    "categorical_columns = ['responsible_person', 'type_problem', 'topic', 'categoria', 'region']\n",
    "datafarme_encoders = CategoricalLabelEncoder(work_dataframe, categorical_columns)\n",
    "\n",
    "datafarme_encoders.get_classes('type_problem')\n",
    "\n",
    "print(datafarme_encoders.decode('type_problem', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dataframe.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 dataframe: pd.DataFrame,\n",
    "                 tokenizer: GPT2Tokenizer,\n",
    "                 max_length=512):\n",
    "        \n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        self.type_ids = torch.tensor(\n",
    "            dataframe['type_problem'].astype(int).tolist()\n",
    "        )\n",
    "\n",
    "        self.source_encodings = tokenizer(\n",
    "            dataframe['source'].tolist(), \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.target_encodings = tokenizer(\n",
    "            dataframe['target'].tolist(), \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.type_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.source_encodings.items()}\n",
    "        item['target_ids'] = self.target_encodings['input_ids'][idx]\n",
    "        item['target_attention_mask'] = self.target_encodings['attention_mask'][idx]\n",
    "        item['type_ids'] = self.type_ids[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель\n",
    "В модели `CustomGPT2Model` логиты будут иметь размеры зависящие от следующих факторов:\n",
    "\n",
    "1. **Количество токенов в входной последовательности**: Обозначим количество токенов как `T`. Это количество токенов, которое вы подаете в модель в `input_ids`. Также это число будет равно длине последовательности, которая используется для генерации выхода.\n",
    "\n",
    "2. **Размер словаря модели GPT-2**: Обозначим размер словаря как `V`. Это количество уникальных токенов, которые модель может генерировать. Этот параметр зависит от конфигурации предобученной модели GPT-2, которую вы используете.\n",
    "\n",
    "### Размеры тензоров в вашей модели:\n",
    "\n",
    "- **`input_ids`**: Тензор размером `[B, T]`, где `B` — размер батча, `T` — количество токенов в каждой последовательности входных данных.\n",
    "\n",
    "- **`type_ids`**: Тензор размером `[B]`, содержащий идентификаторы типов сообщений для каждого элемента в батче. Этот тензор расширяется и повторяется, чтобы соответствовать длине `T`.\n",
    "\n",
    "- **`attention_mask`**: Тензор размером `[B, T]`, который указывает на то, какие токены являются значимыми для каждой последовательности.\n",
    "\n",
    "- **`combined_embeds` и `inputs_embeds`**: Тензоры размером `[B, T, H]`, где `H` — размер скрытого состояния в модели GPT-2.\n",
    "\n",
    "- **`logits`** после применения `lm_head`: Тензор размером `[B, T, V]`, где каждый элемент в тензоре представляет логиты для каждого токена в каждой последовательности батча для каждого возможного токена в словаре.\n",
    "\n",
    "### Как это работает в вашей `forward` функции:\n",
    "\n",
    "1. **Токен-встраивания** (`wte`) и **встраивания типа** комбинируются и проходят через линейный слой (`combined_linear`).\n",
    "\n",
    "2. **Встраивания** передаются в основную модель GPT-2, которая возвращает последние скрытые состояния.\n",
    "\n",
    "3. **Последние скрытые состояния** преобразуются в логиты с помощью `lm_head`, которые представляют собой вероятности следующего токена для каждого токена в последовательности.\n",
    "\n",
    "Таким образом, если вы передаете `input_ids` размером `[5, 5]` (5 последовательностей по 5 токенов каждая), вы получите `logits` размером `[5, 5, V]`, где `V` — размер словаря вашей модели GPT-2. Эти логиты используются для выбора следующего токена в процессе генерации текста или для вычисления потерь во время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model\n",
    "\n",
    "class CustomGPT2Model(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        pretrained_model_name,\n",
    "        num_message_types,\n",
    "        data_path=Path('')\n",
    "    ):\n",
    "        super(CustomGPT2Model, self).__init__()\n",
    "        # Загрузка предобученной модели GPT-2\n",
    "        self.gpt2 = GPT2Model.from_pretrained(pretrained_model_name, cache_dir=data_path)\n",
    "        \n",
    "        # Слой для встраивания типа сообщения\n",
    "        self.type_embedding = nn.Embedding(num_message_types, self.gpt2.config.hidden_size)\n",
    "        \n",
    "        # Дополнительный линейный слой для объединения встраивания типа сообщения и токенов\n",
    "        self.combined_linear = nn.Linear(self.gpt2.config.hidden_size, self.gpt2.config.hidden_size)\n",
    "        # Дополнительный слой для логитов\n",
    "        self.lm_head = nn.Linear(self.gpt2.config.hidden_size, self.gpt2.config.vocab_size)  # Дополнительный слой для логитов\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, type_ids):\n",
    "        # Получение встраиваний токенов\n",
    "        inputs_embeds = self.gpt2.wte(input_ids)  # wte - word token embeddings\n",
    "        \n",
    "        # Получение встраивания для типа сообщения\n",
    "        type_embeds = self.type_embedding(type_ids).unsqueeze(1)  # Расширяем размерности для сложения\n",
    "        \n",
    "        # Сложение встраиваний токенов и типа по всей длине входа\n",
    "        combined_embeds = inputs_embeds + type_embeds.expand(-1, input_ids.size(1), -1)\n",
    "        \n",
    "        # Применяем дополнительный линейный слой\n",
    "        combined_embeds = self.combined_linear(combined_embeds)\n",
    "        \n",
    "        # Передача встраиваний в основную модель GPT-2\n",
    "        outputs = self.gpt2(inputs_embeds=combined_embeds, attention_mask=attention_mask)\n",
    "        \n",
    "        logits = self.lm_head(outputs.last_hidden_state)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plt_img(save_path, fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = save_path / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, tokenizer, weights={'bleu': 0.34, 'rouge': 0.33, 'meteor': 0.33}):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.weights = weights\n",
    "        self.rouge = Rouge()\n",
    "        \n",
    "        self.metrics = []\n",
    "\n",
    "    def evaluate(self, references: list, hypotheses: list):\n",
    "        tokenized_references = [[ref.split()] for ref in references]\n",
    "        tokenized_hypotheses = [hyp.split() for hyp in hypotheses]\n",
    "        \n",
    "        # Вычисление метрик BLEU, ROUGE, METEOR\n",
    "        bleu_score = corpus_bleu(tokenized_references, tokenized_hypotheses)\n",
    "        \n",
    "        rouge_score = self.rouge.get_scores(hypotheses, references, avg=True)['rouge-l']['f']\n",
    "        \n",
    "        list_meteor_score = [meteor_score(refs, hyp) for refs, hyp in zip(tokenized_references, tokenized_hypotheses)]\n",
    "        avg_meteor_score = np.mean(list_meteor_score)\n",
    "\n",
    "        # Словарь с результатами\n",
    "        results = {\n",
    "            'overall': self.weights['bleu'] * bleu_score +\n",
    "                       self.weights['rouge'] * rouge_score +\n",
    "                       self.weights['meteor'] * avg_meteor_score,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge': rouge_score,\n",
    "            'meteor': avg_meteor_score\n",
    "        }\n",
    "        self.metrics.append(results)\n",
    "        return results\n",
    "    \n",
    "    def plot_metrics(self, imgs_path=Path(''), name='special_metrics'):\n",
    "        epochs_range = range(1, len(self.metrics) + 1)\n",
    "        fig = plt.figure(figsize=(15, 6))  # Устанавливаем размер фигуры\n",
    "        \n",
    "        ax12 = plt.subplot(1, 2, 1)\n",
    "        ax12.plot(epochs_range, [m['overall'] for m in self.metrics], label='Overall Score', color='tab:green')\n",
    "        ax12.set_title('Overall Evaluation Score')\n",
    "        ax12.set_xlabel('Epochs')\n",
    "        ax12.set_ylabel('Score')\n",
    "        ax12.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        ax12.legend(loc='upper right')\n",
    "\n",
    "        # Отдельные графики для каждой метрики\n",
    "        ax21 = plt.subplot(1, 2, 2)\n",
    "        ax21.plot(epochs_range, [m['bleu'] for m in self.metrics], label='BLEU Score', color='tab:red')\n",
    "        ax21.plot(epochs_range, [m['rouge'] for m in self.metrics], label='ROUGE Score', color='tab:pink')\n",
    "        ax21.plot(epochs_range, [m['meteor'] for m in self.metrics], label='METEOR Score', color='tab:brown')\n",
    "        ax21.set_title('Individual Metrics')\n",
    "        ax21.set_xlabel('Epochs')\n",
    "        ax21.set_ylabel('Score')\n",
    "        ax21.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        ax21.legend(loc='upper right')      \n",
    "\n",
    "        fig.tight_layout()  # Убедимся, что макет не нарушен\n",
    "        save_plt_img(imgs_path, name)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конфиг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Config:\n",
    "    model_name = 'ai-forever/rugpt3small_based_on_gpt2'\n",
    "    max_length = 32\n",
    "    temperature=0.9\n",
    "    batch_size = 16\n",
    "    test_size=0.1\n",
    "    learning_rate = 1e-5\n",
    "    num_epochs = 3\n",
    "    uniq_name = 'custom_gpt2_model'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    special_eval = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: CustomGPT2Model, \n",
    "        evaluator: Evaluator, \n",
    "        tokenizer,\n",
    "        learning_rate=1e-3,\n",
    "        special_eval=False,\n",
    "        device='cpu',\n",
    "        config: Config=None\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.special_eval = special_eval\n",
    "        self.device = device\n",
    "        self.evaluator =  evaluator\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        \n",
    "        # self.freeze_layers()\n",
    "        # self.print_freezed_layers()\n",
    "\n",
    "    def train(self, train_loader: DataLoader):\n",
    "        \"\"\"\n",
    "        Обучение модели\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0.0\n",
    "        train_bar = tqdm(train_loader, desc='Training', leave=True)\n",
    "        for batch in train_bar:\n",
    "            inputs_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            target_ids = batch['target_ids'].to(self.device)\n",
    "            type_ids = batch['type_ids'].to(self.device)\n",
    "            \n",
    "            logits = self.model(input_ids=inputs_ids, attention_mask=attention_mask, type_ids=type_ids)\n",
    "            loss = self.criterion(\n",
    "                logits.view(-1, self.model.gpt2.config.vocab_size),\n",
    "                target_ids.view(-1)\n",
    "            )\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_bar.set_postfix({'train loss': loss.item()})\n",
    "            \n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "        self.train_losses.append(avg_loss)        \n",
    "        return avg_loss\n",
    "            \n",
    "\n",
    "    def eval(self, eval_loader: DataLoader):\n",
    "        self.model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_bar = tqdm(eval_loader, desc=f'Evaluate', leave=True)\n",
    "        with torch.no_grad():\n",
    "            for batch in eval_bar:\n",
    "                inputs_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                target_ids = batch['target_ids'].to(self.device)\n",
    "                type_ids = batch['type_ids'].to(self.device)\n",
    "                \n",
    "                logits = self.model(input_ids=inputs_ids, attention_mask=attention_mask, type_ids=type_ids)\n",
    "\n",
    "                loss = self.criterion(\n",
    "                    logits.view(-1, self.model.gpt2.config.vocab_size),\n",
    "                    target_ids.view(-1)\n",
    "                )                          \n",
    "                eval_loss += loss.item()\n",
    "                eval_bar.set_postfix({'eval loss': loss.item()})\n",
    "                \n",
    "            avg_loss = eval_loss / len(eval_loader)\n",
    "            self.val_losses.append(avg_loss)    \n",
    "        \n",
    "        self.calculate_special_eval(eval_loader, self.config.max_length, self.config.temperature)\n",
    "        \n",
    "        return avg_loss\n",
    "            \n",
    "    def fit(self, epochs, train_loader: DataLoader, eval_loader: DataLoader):\n",
    "        epoch_bar = tqdm(range(epochs), desc=f'Fit ', leave=True)\n",
    "        for epoch in epoch_bar:\n",
    "             # Вызов функции обучения и получение средней потери на тренировочных данных\n",
    "            train_avg_loss = self.train(train_loader)\n",
    "            # Вызов функции оценки и получение средней потери на валидационных данных\n",
    "            eval_avg_loss = self.eval(eval_loader)\n",
    "            \n",
    "            # Обновление прогресс-бара с текущей эпохой и значениями потерь\n",
    "            epoch_bar.set_postfix({\n",
    "                'Epoch': epoch,\n",
    "                'Train Loss': f\"{train_avg_loss:.4f}\",\n",
    "                'Eval Loss': f\"{eval_avg_loss:.4f}\"\n",
    "            })\n",
    "        \n",
    "    def calculate_special_eval(self, eval_loader: DataLoader, max_length, temperature):\n",
    "        if self.special_eval and self.evaluator:\n",
    "            self.model.eval()\n",
    "            eval_bar = tqdm(eval_loader, desc=f'Special evaluate', leave=True)\n",
    "            hypotheses = []\n",
    "            references = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in eval_bar:\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    target_ids = batch['target_ids'].to(self.device)\n",
    "                    type_ids = batch['type_ids'].to(self.device)\n",
    "                    \n",
    "                    hypotheses += self._generate_text_sampling(input_ids, attention_mask, type_ids, max_length, temperature)\n",
    "                    references += self.decode_tensor(target_ids)\n",
    "            self.evaluator.evaluate(hypotheses=hypotheses, references=references)\n",
    "        \n",
    "    def generate_text(self, text, type_id, max_length=25, method='argmax', temperature=0.9, top_k=10, top_p=0.9):\n",
    "        # Токенизация входного текста\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        input_ids = inputs['input_ids'].to(self.device)\n",
    "        attention_mask = inputs['attention_mask'].to(self.device)\n",
    "        type_ids = torch.tensor([type_id]).to(self.device)\n",
    "        \n",
    "        if method == 'argmax':\n",
    "            return self._generate_text_argmax(input_ids, attention_mask, type_ids, max_length)\n",
    "        elif method == 'sampling':\n",
    "            return self._generate_text_sampling(input_ids, attention_mask, type_ids, max_length, temperature)\n",
    "        elif method == 'top_k':\n",
    "            return self._generate_text_top_k(input_ids, attention_mask, type_ids, max_length, k=top_k)\n",
    "        elif method == 'top_p':\n",
    "            return self._generate_text_top_p(input_ids, attention_mask, type_ids, max_length, p=top_p)\n",
    "            \n",
    "    def _generate_text_sampling(self, input_ids, attention_mask, type_ids, max_length=25, temperature=0.9):\n",
    "        self.model.eval() \n",
    "        generated_sequence = input_ids   \n",
    "        \n",
    "        # Начальная длина входных данных\n",
    "        start_gen_index = input_ids.size(1) \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                outputs = self.model(input_ids=generated_sequence, attention_mask=attention_mask, type_ids=type_ids)\n",
    "                logits = outputs[:, -1, :]                \n",
    "                # Применяем температуру для управления случайностью выборки\n",
    "                logits = logits / temperature\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                next_token_id = torch.multinomial(probabilities, num_samples=1)                \n",
    "                generated_sequence = torch.cat((generated_sequence, next_token_id), dim=-1)\n",
    "                \n",
    "                # Обновление attention_mask\n",
    "                attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.size(0), 1), device=self.device)], dim=1)\n",
    "\n",
    "        # Возвращаем только сгенерированную часть\n",
    "        generated_part = generated_sequence[:, start_gen_index:]\n",
    "        return self.decode_tensor(generated_part)\n",
    "    \n",
    "    def _generate_text_argmax(self, input_ids, attention_mask, type_ids, max_length=25):\n",
    "        self.model.eval()\n",
    "        generated_sequence = input_ids\n",
    "\n",
    "        # Начальная длина входных данных\n",
    "        start_gen_index = input_ids.size(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                outputs = self.model(input_ids=generated_sequence, attention_mask=attention_mask, type_ids=type_ids)\n",
    "                                \n",
    "                logits = outputs[:, -1, :]\n",
    "                                \n",
    "                next_token_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                generated_sequence = torch.cat((generated_sequence, next_token_id), dim=-1)\n",
    "                \n",
    "                # Обновление attention_mask\n",
    "                attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.size(0), 1), device=self.device)], dim=1)\n",
    "        \n",
    "        # Возвращаем только сгенерированную часть\n",
    "        generated_part = generated_sequence[:, start_gen_index:]\n",
    "        return self.decode_tensor(generated_part)\n",
    "    \n",
    "    def _generate_text_top_k(self, input_ids, attention_mask, type_ids, max_length=25, k=10):\n",
    "        self.model.eval()\n",
    "        generated_sequence = input_ids\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                outputs = self.model(input_ids=generated_sequence, attention_mask=attention_mask, type_ids=type_ids)\n",
    "                logits = outputs[:, -1, :]\n",
    "                # Применяем Top-k sampling\n",
    "                top_k = torch.topk(logits, k=k, dim=-1)\n",
    "                probabilities = torch.softmax(top_k.values, dim=-1)\n",
    "                next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "                next_token_id = top_k.indices.gather(-1, next_token_id)\n",
    "                \n",
    "                generated_sequence = torch.cat((generated_sequence, next_token_id), dim=-1)\n",
    "                attention_mask = torch.cat([attention_mask, torch.ones((1, 1), device=self.device)], dim=1)\n",
    "\n",
    "        return self.decode_tensor(generated_sequence[:, input_ids.size(1):])\n",
    "\n",
    "    def _generate_text_top_p(self, input_ids, attention_mask, type_ids, max_length=25, p=0.9):\n",
    "        self.model.eval()\n",
    "        generated_sequence = input_ids\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                outputs = self.model(input_ids=generated_sequence, attention_mask=attention_mask, type_ids=type_ids)\n",
    "                logits = outputs[:, -1, :]\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                # Удаление токенов с суммой вероятностей выше p\n",
    "                sorted_indices_to_remove = cumulative_probs > p\n",
    "                # Оставляем хотя бы один токен, если все превышают p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = False\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = float('-inf')\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "                generated_sequence = torch.cat((generated_sequence, next_token_id), dim=-1)\n",
    "                attention_mask = torch.cat([attention_mask, torch.ones((1, 1), device=self.device)], dim=1)\n",
    "\n",
    "        return self.decode_tensor(generated_sequence[:, input_ids.size(1):])\n",
    "    \n",
    "    def decode_tensor(self, sequences: Tensor):\n",
    "        decoded_texts = []\n",
    "        for sequence in sequences:\n",
    "            decoded_text = self.tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "            decoded_texts.append(decoded_text)\n",
    "        return decoded_texts\n",
    "\n",
    "    def plot_main_metrics(self, imgs_path=Path(''), name='train_metrics'):\n",
    "        epochs_range = range(1, len(self.val_losses) + 1)\n",
    "        fig = plt.figure(figsize=(15, 6))  # Устанавливаем размер фигуры\n",
    "        \n",
    "        ax11 = plt.subplot()\n",
    "        ax11.plot(epochs_range, self.train_losses, label='Train loss', color='tab:red')\n",
    "        ax11.plot(epochs_range, self.val_losses, label='Evaluate loss', color='tab:blue')\n",
    "        ax11.set_title('Losses over Epochs')\n",
    "        ax11.set_xlabel('Epochs')\n",
    "        ax11.set_ylabel('Loss')\n",
    "        ax11.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "        ax11.legend(loc='upper right')     \n",
    "\n",
    "        fig.tight_layout()  # Убедимся, что макет не нарушен\n",
    "        save_plt_img(imgs_path, name)  # extra code\n",
    "        plt.show()\n",
    "            \n",
    "    def freeze_layers(self, num_trainable_blocks=2):        \n",
    "        total_blocks = len(self.model.gpt2.h)  # h - это список всех блоков трансформера в модели GPT-2\n",
    "\n",
    "        # Заморозка слоёв в начальных блоках\n",
    "        for i, block in enumerate(self.model.gpt2.h):\n",
    "            if i < total_blocks - num_trainable_blocks:\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # # Если у вас есть другие специфические слои, которые нужно обучать, размораживаем их\n",
    "        # for param in model.combined_linear.parameters():\n",
    "        #     param.requires_grad = True\n",
    "    \n",
    "    def print_freezed_layers(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            print(f\"{name} is trainable: {param.requires_grad}\")\n",
    "    \n",
    "    def save_model(self, file_path: Path):\n",
    "        # Словарь для сохранения\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'config': self.config\n",
    "        }\n",
    "        torch.save(checkpoint, file_path)\n",
    "        print(f\"Model saved to {file_path}\")\n",
    "    \n",
    "    def load_model(self, file_path: Path):\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.model.to(self.device)  # Убедитесь, что модель перенесена на нужное устройство\n",
    "        print(f\"Model loaded from {file_path}\")\n",
    "        \n",
    "    def render_graaph(self, output, imgs_path=Path(''), name='neural_network_graph'):\n",
    "        # Визуализация графа\n",
    "        dot = make_dot(output, params=dict(self.model.named_parameters()))\n",
    "        dot.attr('node', style='filled', color='lightblue')\n",
    "        dot.attr('edge', style='dashed', color='grey')                   \n",
    "        dot.format = 'png'  # Устанавливаем формат файла\n",
    "        # Можно установить разрешение в dpi (точек на дюйм), например 300 dpi\n",
    "        dot.attr(dpi='300')\n",
    "        dot.render(filename=name,\n",
    "                   directory=imgs_path)  # Сохраняем граф в файл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Про генерацию текста\n",
    "### Что такое параметр temperature в генерации текста?\n",
    "\n",
    "Параметр `temperature` используется для управления случайностью выбора токенов в процессе генерации текста. Он модифицирует распределение вероятностей следующих токенов, возвращаемых моделью, и может существенно влиять на разнообразие и неожиданность генерируемого текста:\n",
    "\n",
    "- **Низкая температура** (ближе к 0) делает распределение более уверенным и предсказуемым, сосредотачиваясь на наиболее вероятных токенах. Это может привести к повторениям или более банальному тексту.\n",
    "- **Высокая температура** делает распределение более равномерным и увеличивает шансы менее вероятных токенов быть выбранными, что ведет к более разнообразному и творческому тексту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "\n",
    "class TrainingManager:\n",
    "    def __init__(self, \n",
    "        dataframe: pd.DataFrame,\n",
    "        categorical_encoder: CategoricalLabelEncoder,\n",
    "        config: Config,\n",
    "        data_path: Path=DATA_PATH,\n",
    "        imgs_path: Path=IMAGES_PATH \n",
    "    ):\n",
    "        \n",
    "        self.uniq_name = config.uniq_name\n",
    "        self.data_path = data_path \n",
    "        self.imgs_path = imgs_path    \n",
    "        self.dataframe = dataframe\n",
    "        self.categorical_encoder = categorical_encoder\n",
    "                \n",
    "        self.config = config\n",
    "        \n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(config.model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Разбиение данных на обучающую и тестовую выборки\n",
    "        train_data, eval_data = train_test_split(\n",
    "            self.dataframe[['type_problem', 'source', 'target']], test_size=0.1\n",
    "        )\n",
    "\n",
    "        # Создание объектов Dataset\n",
    "        train_dataset = CustomDataset(dataframe=train_data, tokenizer=self.tokenizer, max_length=config.max_length)\n",
    "        eval_dataset = CustomDataset(dataframe=eval_data, tokenizer=self.tokenizer, max_length=config.max_length)\n",
    "\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=config.batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        self.eval_dataloader = DataLoader(\n",
    "            eval_dataset, \n",
    "            batch_size=config.batch_size, \n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        self.model = CustomGPT2Model(\n",
    "            pretrained_model_name=config.model_name,\n",
    "            num_message_types=len(self.categorical_encoder.get_classes('type_problem')),\n",
    "            data_path=self.data_path\n",
    "        )\n",
    "        \n",
    "        evaluator = Evaluator(\n",
    "            self.tokenizer \n",
    "        )\n",
    "        \n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            evaluator=evaluator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            learning_rate=config.learning_rate,\n",
    "            device=config.device,\n",
    "            config=config,\n",
    "            special_eval=config.special_eval           \n",
    "        )  \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Запуск процесса обучения.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.trainer.fit(self.config.num_epochs, self.train_dataloader, self.eval_dataloader)\n",
    "        \n",
    "    def plot_main_metrics(self):\n",
    "        name = self.uniq_name + '_main_metrics'\n",
    "        self.trainer.plot_main_metrics(self.imgs_path, name)\n",
    "        \n",
    "    def plot_special_metrics(self):\n",
    "        name = self.uniq_name + '_special_metrrics'\n",
    "        self.trainer.evaluator.plot_metrics(self.imgs_path, name)\n",
    "    \n",
    "    def generate_text(self, text, type_id, max_length=25, method='samplig', **kwargs):\n",
    "        return self.trainer.generate_text(text, type_id, max_length, method=method, **kwargs)[0]\n",
    "\n",
    "    def save(self):\n",
    "        self.trainer.save_model(self.data_path / (self.uniq_name + '.pth'))\n",
    " \n",
    "    def load(self):\n",
    "        self.trainer.load_model(self.data_path / (self.uniq_name + '.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dataframe = work_dataframe[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "config.learning_rate = 1e-3\n",
    "config.num_epochs = 3\n",
    "config.max_length = 64\n",
    "config.temperature = 0.9\n",
    "config.batch_size = 16\n",
    "config.test_size = 0.1\n",
    "config.uniq_name = 'custom_gpt2_model'\n",
    "config.special_eval = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_manager = TrainingManager(\n",
    "    work_dataframe,\n",
    "    datafarme_encoders,\n",
    "    config,\n",
    "    data_path=DATA_PATH,\n",
    "    imgs_path=IMAGES_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_manager.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_manager.plot_main_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_manager.plot_special_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_manager.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предикт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_train_data: pd.DataFrame = training_manager.train_dataloader.dataset.dataframe\n",
    "testing_train_data = testing_train_data.reset_index()\n",
    "testing_train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_message = testing_train_data['type_problem'][0]\n",
    "source = testing_train_data['source'][0]\n",
    "target = testing_train_data['target'][0]\n",
    "\n",
    "print(datafarme_encoders.decode('type_problem', type_message))\n",
    "\n",
    "print('Source - ', source)\n",
    "print('Target - ', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = training_manager.generate_text(source, type_message, max_length=config.max_length, method='argmax')\n",
    "print('argmax -', generated_text)\n",
    "\n",
    "# generated_text = training_manager.generate_text(source, type_message, max_length=config.max_length, method='sampling', temperature=0.7,)\n",
    "# print('sampling -', generated_text)\n",
    "\n",
    "# generated_text = training_manager.generate_text(source, type_message, max_length=config.max_length, method='top_k', top_k=11)\n",
    "# print('top_k -', generated_text)\n",
    "\n",
    "# generated_text = training_manager.generate_text(source, type_message, max_length=config.max_length, method='top_p', top_p=0.9)\n",
    "# print('top_p -', generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_metrics = training_manager.trainer.evaluator.evaluate(\n",
    "    hypotheses=[generated_text],\n",
    "    references=[target]\n",
    ")\n",
    "print(special_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_metrics = training_manager.trainer.evaluator.evaluate(\n",
    "    hypotheses=['Привет'],\n",
    "    references=['Привки'],\n",
    ")\n",
    "print(special_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
