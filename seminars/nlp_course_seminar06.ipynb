{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2EasHxOX_LJ"
      },
      "source": [
        "# Deep learning tips and tricks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2CSEimwX_LU"
      },
      "source": [
        "In this notebook we will cover a couple of tips and tricks for tweaking a neural text classifier. We will use an LSTM model for our experiments. We use torch version 1.4.\n",
        "The code is inspired by [this](https://github.com/lukysummer/Movie-Review-Sentiment-Analysis-LSTM-Pytorch/blob/master/sentiment_analysis_LSTM.py) repository."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "seed = 0\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "sY1AdxaZoqZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R6tP8mdX_LV"
      },
      "source": [
        "### 1. LOAD THE TRAINING TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxDiE0w9X_LW"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64zxqttYX_LY"
      },
      "outputs": [],
      "source": [
        "from nltk import download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8UTNnQ5X_LZ",
        "outputId": "3de96f98-9d2d-4299-808d-262224b0e755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "download(\"movie_reviews\", download_dir=\"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHq58g0eX_LZ"
      },
      "outputs": [],
      "source": [
        "movies = load_files(\"./data/corpora/movie_reviews\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTnJpiAoX_La"
      },
      "outputs": [],
      "source": [
        "reviews, encoded_labels = [review.decode() for review in movies.data], movies.target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "c = list(zip(reviews, encoded_labels))\n",
        "\n",
        "random.shuffle(c)\n",
        "\n",
        "reviews, encoded_labels = zip(*c)"
      ],
      "metadata": {
        "id": "m4BhAQ3q2bXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "zVH42QkBvjWW",
        "outputId": "8af0b112-6e2b-4d4b-dc67-7abb7893d41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"vannesa kensington : `austin , do you smoke after sex ? ' austin powers : `i don't know baby , i've never looked ! ' \\nand so begins our journey into the most anticipated sequel of the summer season . \\naustin powers 2 , the sequel to the sleeper hit of 1997 , is filled to the brim with uproarious sight gags and lurid toilet jokes that will make you keel over with hilarity . \\nthe mind of mike myers is obviously a very bizarre place . \\nmyers returns as the swinging 60's spy and his arch-nemesis , the bald headed dr . evil , who is given much of the spotlight here . \\nthere's an early scene in which dr . evil and his son scott ( seth green ) appear on a jerry springer segment entitled `my dad is evil and wants to take over the world' , hosted by springer himself . \\nmost of these talk-show gags , spoofing everything from oprah to regis and kathie lee , are no longer as funny as they once were . \\nhappily , this is an exception , especially when a fight breaks out between dr . evil and the other guests . \\n ( sample dialogue- `come back here you mother #@%$^ ! ! \\nyou want a piece of me ? ' ) \\nthe audience was in stitches . \\nthere are many good things about this sequel . \\ndr . evil is now assisted by mini-me ( verne troyer ) , a pint-sized clone who is the result of an experiment gone awry . \\nmini-me is the most perfect new character that any fan could ask for , and troyer does a terrific job of mimicking everything that his big brother does , pinkie and all . \\nmindy sterling returns as frau farbissina , evil's loud-mouth assassin sidekick , and here we are treated to a brief romantic liaison between the two in an underground bedroom chamber . \\nthis works surprisingly well . \\nafterward , there's even a meeting at the coffee machine , where their awkwardness is expressed in a scene you would expect in any normal office environment . \\noh , yes . . . \\nthe plot . \\nafter a short honeymoon with vanessa kensington ( elizabeth hurley ) that goes disastrously wrong , powers is back on the case to thwart the diabolical plans of dr . evil . \\nin the original , the subject of the day was cryogenic freezing that enabled the characters to hop between two time periods . \\nhere , it's time travel . \\ndr . evil has this crazy plan to steal austin's mojo ( the source of his sexual powers ) , so he jumps through a time portal back into 1969 when powers is still is his freezing chamber . \\nthere , he employs a grotesque 600-pound scottish assassin named fat bastard ( also myers ) to commandeer the mojo , so that our future hero is left completely shagless . \\nmeanwhile , austin is informed of evil's wrong-doing by basil exposition ( michael york ) , and is supplied with his very own time-traveling device in the form of a volkswagen beetle . \\nonce back in 1969 , powers meets a foxy cia operative named felicity shagwell ( heather graham ) , and learns of dr . evil's plans to destroy washington dc with a lazer on the moon . \\nin one of the many uproarious jabs at star wars , evil calls the moon base a `death star' . \\nif there ever was a movie that knew from the start it was a complete joke , it's austin powers : the spy who shagged me . \\nap2 has many hilarious moments , much more so , than the original austin powers . \\nbut what it doesn't have is the confident support system that the first boasted , and the jokes are now more scattershot and hit-and-miss . \\nwhile myers has a knack for delivering clever , inspired gags , he sometimes doesn't know when to call it quits . \\nthe hilarious scene in the original involving mustafa ( will ferrell ) and his refusal to die is played out here in a disappointing and tired manner . \\nrobert wagner , as dr . evil's #2 man , has only one scene early on in the film , where he lacks all the focus of the character . \\none nice surprise is rob lowe , as the young #2 , who patterns wagner's voice and movements right down to a tee . \\nthere are more recycled jokes from the predecessor , including kristen johnston as ivana humpalot ( an unfunny rendition of alotta fagina ) . \\nbut myers has clearly put so much dedication into this project , and many bits are side-splittingly funny . \\ndon't miss an extended visual gag in a tent , where the shadows leave much to the imagination . . . . \\nof the new characters , graham doesn't leave much of an impression . \\nher performance is merely okay , and she lacks the gung-ho enthusiasm that made elizabeth hurley so enjoyable before her . \\nand as for fat bastard . . . \\nwell , he provides one or two hilarious moments , but all of the obesity jokes and bathroom talk go too far . \\nbut my most severe complaint : mr . bigglesworth was only in one scene ! ! \\nbut , with gritted teeth , i managed to get over that . \\nthis sequel will not leave fans disappointed ; it's more often than not a laugh riot . \\nand , i look optimistically to the future , where i foresee more installments to the austin powers collection . \\nif myers can up the quota of fresh ideas , we should be in store for much more . \\n ( c ) 1999 , jamey hughton . . . . . . . . . . . . . . . . . . . . . . . . \\njamey hughton ( 15 ) has written a weekly column in the starphoenix , saskatoon , sk since november , 1997 . \\nhe was a 1999 writing finalist in the canadian ytv achievement awards . \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdGarm3rc0Uy",
        "outputId": "7320903d-844e-4228-dfcd-82797d348c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxfXyBDNX_Lb"
      },
      "source": [
        "### 2. TEXT PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBJi5StkX_Lc"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "import re\n",
        "\n",
        "word_re = re.compile(r\"\\b[a-z]{2,}\\b\")\n",
        "\n",
        "def tokenize(text):\n",
        "    processed_text = \"\".join(ch for ch in text.lower() if ch not in punctuation)\n",
        "    processed_text = processed_text.replace(\"\\n\", \" \")\n",
        "    return word_re.findall(processed_text)\n",
        "\n",
        "def flatten(tokenized_texts):\n",
        "    return [word for text in tokenized_texts for word in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udapnCpIX_Ld"
      },
      "outputs": [],
      "source": [
        "all_reviews = list(map(tokenize, reviews))\n",
        "all_words = flatten(all_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMIOAA53X_Ld"
      },
      "source": [
        "### 3. CREATE DICTIONARIES & ENCODE REVIEWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z39ExWoBX_Le"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "word_list = sorted(word_counts, key=lambda k: word_counts[k], reverse = True)\n",
        "vocab_to_int = {word:idx+1 for idx, word in enumerate(word_list)}\n",
        "int_to_vocab = {idx:word for word, idx in vocab_to_int.items()}\n",
        "encoded_reviews = [[vocab_to_int[word] for word in review] for review in all_reviews]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVKlf36KX_Le"
      },
      "source": [
        "### 4. CHECK LABELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTxF7BawX_Le"
      },
      "outputs": [],
      "source": [
        "assert len(encoded_reviews) == len(encoded_labels), \"# of encoded reviews & encoded labels must be the same!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOO0gtkbX_Lf"
      },
      "source": [
        "### 5. GET RID OF LENGTH-0 REVIEWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMjIIrgLX_Lf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "encoded_labels = np.array([label for idx, label in enumerate(encoded_labels) if len(encoded_reviews[idx]) > 0])\n",
        "encoded_reviews = [review for review in encoded_reviews if len(review) > 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry9LlELrX_Lf"
      },
      "source": [
        "### 6. MAKE ALL REVIEWS SAME LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVh8EoFaX_Lf"
      },
      "outputs": [],
      "source": [
        "def pad_text(encoded_reviews, seq_length):\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "    for review in encoded_reviews:\n",
        "        if len(review) >= seq_length:\n",
        "            reviews.append(review[:seq_length])\n",
        "        else:\n",
        "            reviews.append([0] * (seq_length - len(review)) + review)\n",
        "\n",
        "    return np.array(reviews)\n",
        "\n",
        "padded_reviews = pad_text(encoded_reviews, seq_length=200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_reviews[42]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfIBYwRdgaz6",
        "outputId": "82ac8423-c920-4aef-a389-f2588b26d339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   82,   221,  1655,   183,    46,    58,    74,   559,  4335,\n",
              "        6955,   820,  8195,     6,  3156,     4,   157,    16,   851,\n",
              "         319,  2865,  6644,  1740,   538,   101,    49,   725,   850,\n",
              "          13,    24,  4636,  5349,  1868,  8125,  4364,    17,   355,\n",
              "           2,   327,  1128,     3,  6335,  8597,    19,  8671,     6,\n",
              "           1,   886,  1868,  1555,   232,   477,    44,     4,   538,\n",
              "           1,  7707,  2096,   139,    34,    62, 21117,     2,  4775,\n",
              "         616,     1,   486,   690,  1669,     1,  1393,     5,    17,\n",
              "          43,    83,     4,  2239,     1,   355,     9,    81,    36,\n",
              "          34,    18,   424,   454,    17,    61,   554,   759,   116,\n",
              "           5,     1,    96,     3,     1,  2406,    65,   682,     3,\n",
              "        2865,  6645,  3403,   343,     9,     1,  3169,  8671, 12042,\n",
              "          26,     5,  1978,    27,    32,  2503,   261,    10,    32,\n",
              "         731,  2288,   182,    10,  2295, 21118,    19,  6336,     6,\n",
              "         475,     4,   260,   327,  2304,   355,   726,    39,   157,\n",
              "          47,   675,  3481,     2,  1868,    30,    66, 21119,     4,\n",
              "         289,  1014,    10,     1,   260,     3,     1,   675,  2592,\n",
              "        3141,  2673,  4607, 27677,  3532,  9329, 27678,     2,  1655,\n",
              "         820,  8195, 12042,  1197,     8,     5,   994,  2865,    26,\n",
              "          30,    66,  4636,  5349,    19, 27679,    73,     4,    20,\n",
              "          40,  5161,     2, 27680,     6,   475,     4,    20,   369,\n",
              "           9,  1393,  2240,    30,     4,   176,   123,     7,  1108,\n",
              "         444,    46])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTyqcjt7X_Lg"
      },
      "source": [
        "### 7. SPLIT DATA & GET (REVIEW, LABEL) DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e19t8p9BX_Lg"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.8\n",
        "valid_ratio = (1 - train_ratio)/2\n",
        "total = padded_reviews.shape[0]\n",
        "train_cutoff = int(total * train_ratio)\n",
        "valid_cutoff = int(total * (1 - valid_ratio))\n",
        "\n",
        "train_x, train_y = torch.from_numpy(padded_reviews[:train_cutoff]), torch.from_numpy(encoded_labels[:train_cutoff])\n",
        "valid_x, valid_y = torch.from_numpy(padded_reviews[train_cutoff:valid_cutoff]), torch.from_numpy(encoded_labels[train_cutoff:valid_cutoff])\n",
        "test_x, test_y = torch.from_numpy(padded_reviews[valid_cutoff:]), torch.from_numpy(encoded_labels[valid_cutoff:])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_data = TensorDataset(train_x, train_y)\n",
        "valid_data = TensorDataset(valid_x, valid_y)\n",
        "test_data = TensorDataset(test_x, test_y)\n",
        "\n",
        "BATCH_SIZE = 50\n",
        "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle = True)\n",
        "valid_loader = DataLoader(valid_data, batch_size = BATCH_SIZE, shuffle = True)\n",
        "test_loader = DataLoader(test_data, batch_size = BATCH_SIZE, shuffle = True)"
      ],
      "metadata": {
        "id": "RSRt2XaKvY3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVmejKwSX_Lh"
      },
      "source": [
        "### 8. DEFINE THE LSTM MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IBwlUylX_Li"
      },
      "source": [
        "During the model definition step, we might re-implement model weight initialisation, apply another tricks such as adding various types of dropout to the needed layers etc. There is a noteworthy [discussion](https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch) on wheather one should initialize weights manually or not, and, if yes, how? The functions that implement various initialisation methods are located in the `torch.nn.init` module.\n",
        "\n",
        "\n",
        "![](https://www.researchgate.net/publication/334268507/figure/fig8/AS:788364231987201@1564972088814/The-structure-of-the-Long-Short-Term-Memory-LSTM-neural-network-Reproduced-from-Yan.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = n_hidden\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.embedding = nn.Embedding(n_vocab, n_embed, padding_idx=0)\n",
        "\n",
        "        # input_size – The number of expected features in the input x\n",
        "        # hidden_size – The number of features in the hidden state h\n",
        "        # num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
        "        # bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
        "        # batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
        "        # dropout – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
        "        # bidirectional – If True, becomes a bidirectional LSTM. Default: False\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=n_embed, hidden_size=self.hidden_dim, num_layers=self.n_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        h = torch.zeros((self.n_layers, x.size(0), self.hidden_dim)).to(device)\n",
        "        c = torch.zeros((self.n_layers, x.size(0), self.hidden_dim)).to(device)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(h)\n",
        "        torch.nn.init.xavier_normal_(c)\n",
        "\n",
        "        out = self.dropout(self.embedding(x)) # (batch_size, seq_length, n_embed) -> 50, 200, 100\n",
        "        out, (hidden, cell) = self.lstm(out, (h,c)) # (batch_size, seq_length, n_hidden) -> 50, 200, 64\n",
        "        out = self.dropout(out)\n",
        "        # taking last element of the sequence\n",
        "        out = torch.sigmoid(self.fc1(out[:,-1,:])) # (batch_size, output) -> 50, 1\n",
        "        return out"
      ],
      "metadata": {
        "id": "u9YRMbanuXIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eaDN1OAX_Ll"
      },
      "source": [
        "### 9. INSTANTIATE THE MODEL W/ HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvYmPs_WX_Ll"
      },
      "outputs": [],
      "source": [
        "n_vocab = len(vocab_to_int)\n",
        "n_embed = 100\n",
        "n_hidden = 64\n",
        "n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
        "n_layers = 1\n",
        "\n",
        "model = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd2Gukmmk917",
        "outputId": "fdf62ecc-b828-4005-839b-753f316e3c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(46694, 100, padding_idx=0)\n",
            "  (lstm): LSTM(100, 64, batch_first=True)\n",
            "  (fc1): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byUVkGZYX_Ll"
      },
      "source": [
        "### 10. DEFINE LOSS & OPTIMIZER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1Wei76BX_Lm"
      },
      "source": [
        "L2-regularization is already included into the optimizer. The `weight_decay` parameter is responsible for controlling its intensity.\n",
        "\n",
        "[BCELoss vs CrossEntropyLoss](https://medium.com/dejunhuang/learning-day-57-practical-5-loss-function-crossentropyloss-vs-bceloss-in-pytorch-softmax-vs-bd866c8a0d23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj0oBKguX_Lm"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.1e-2, weight_decay=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRFfCP5IX_Lm"
      },
      "source": [
        "### 11. TRAIN THE NETWORK!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6e2ytfOX_Ln"
      },
      "source": [
        "To prevent the exploding gradient problem in LSTM/RNN we use the `clip_grad_norm_` function, that takes the `clip` parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_R4Os0WX_Ln",
        "outputId": "33a5c70b-7698-405c-afbf-504d558fe5dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20 Training Loss: 0.6962 Validation Loss: 0.6962 Validation accuracy: 0.4750\n",
            "Epoch: 2/20 Training Loss: 0.6931 Validation Loss: 0.6963 Validation accuracy: 0.4350\n",
            "Epoch: 3/20 Training Loss: 0.6899 Validation Loss: 0.6965 Validation accuracy: 0.4650\n",
            "Epoch: 4/20 Training Loss: 0.6883 Validation Loss: 0.6948 Validation accuracy: 0.5050\n",
            "Epoch: 5/20 Training Loss: 0.6871 Validation Loss: 0.6912 Validation accuracy: 0.5150\n",
            "Epoch: 6/20 Training Loss: 0.6820 Validation Loss: 0.6907 Validation accuracy: 0.4550\n",
            "Epoch: 7/20 Training Loss: 0.6754 Validation Loss: 0.6950 Validation accuracy: 0.5100\n",
            "Epoch: 8/20 Training Loss: 0.6757 Validation Loss: 0.6958 Validation accuracy: 0.5150\n",
            "Epoch: 9/20 Training Loss: 0.6717 Validation Loss: 0.7067 Validation accuracy: 0.4800\n",
            "Epoch: 10/20 Training Loss: 0.6671 Validation Loss: 0.7115 Validation accuracy: 0.4350\n",
            "Epoch: 11/20 Training Loss: 0.6599 Validation Loss: 0.7298 Validation accuracy: 0.5050\n",
            "Epoch: 12/20 Training Loss: 0.6521 Validation Loss: 0.7339 Validation accuracy: 0.4900\n",
            "Epoch: 13/20 Training Loss: 0.6572 Validation Loss: 0.7357 Validation accuracy: 0.4750\n",
            "Epoch: 14/20 Training Loss: 0.6485 Validation Loss: 0.7400 Validation accuracy: 0.5400\n",
            "Epoch: 15/20 Training Loss: 0.6486 Validation Loss: 0.7361 Validation accuracy: 0.5700\n",
            "Epoch: 16/20 Training Loss: 0.6416 Validation Loss: 0.7483 Validation accuracy: 0.5850\n",
            "Epoch: 17/20 Training Loss: 0.6405 Validation Loss: 0.7360 Validation accuracy: 0.4600\n",
            "Epoch: 18/20 Training Loss: 0.6270 Validation Loss: 0.7541 Validation accuracy: 0.4750\n",
            "Epoch: 19/20 Training Loss: 0.6255 Validation Loss: 0.7593 Validation accuracy: 0.5100\n",
            "Epoch: 20/20 Training Loss: 0.6241 Validation Loss: 0.7381 Validation accuracy: 0.5100\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 20\n",
        "clip = 5  # gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        output = model(inputs)\n",
        "\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        train_losses.append(loss.cpu().item())\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "    ######################\n",
        "    ##### VALIDATION #####\n",
        "    ######################\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        valid_losses = []\n",
        "        num_correct = 0\n",
        "        for v_inputs, v_labels in valid_loader:\n",
        "            v_inputs, v_labels = v_inputs.to(device), v_labels.to(device)\n",
        "\n",
        "\n",
        "            v_output = model(v_inputs)\n",
        "            v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
        "            valid_losses.append(v_loss.item())\n",
        "            preds = torch.round(v_output.squeeze())\n",
        "            correct_tensor = preds.eq(labels.float().view_as(preds))\n",
        "            correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "            num_correct += np.sum(correct)\n",
        "\n",
        "        print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
        "              \"Training Loss: {:.4f}\".format(np.mean(train_losses)),\n",
        "              \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)),\n",
        "              f\"Validation accuracy: {num_correct/len(valid_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOBJCieBX_Lp"
      },
      "source": [
        "### 12. TEST THE TRAINED MODEL ON THE TEST SET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgebFtg9X_Lp",
        "outputId": "77a02b4a-8dc3-4a1d-874b-1d8d836cacd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7333\n",
            "Test Accuracy: 0.51\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "model.to(device)\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        test_output = model(inputs)\n",
        "        loss = criterion(test_output.squeeze(), labels.float())\n",
        "        test_losses.append(loss.item())\n",
        "\n",
        "        preds = torch.round(test_output.squeeze())\n",
        "        correct_tensor = preds.eq(labels.float().view_as(preds))\n",
        "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "        num_correct += np.sum(correct)\n",
        "\n",
        "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
        "print(\"Test Accuracy: {:.2f}\".format(num_correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-SoWsgKX_Lp"
      },
      "source": [
        "### 13. TEST THE TRAINED MODEL ON A RANDOM SINGLE REVIEW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhcC2t6HX_Lp"
      },
      "outputs": [],
      "source": [
        "def predict(net, review, seq_length = 200):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    words = tokenize(review)\n",
        "    encoded_words = [vocab_to_int[word] for word in words]\n",
        "    padded_words = pad_text([encoded_words], seq_length)\n",
        "    padded_words = torch.from_numpy(padded_words.reshape(1, -1)).to(device)\n",
        "\n",
        "    if(len(padded_words) == 0):\n",
        "        \"Your review must contain at least 1 word!\"\n",
        "        return None\n",
        "\n",
        "    net.eval()\n",
        "    output = model(padded_words)\n",
        "    pred = torch.round(output.squeeze())\n",
        "    msg = \"This is a positive review.\" if pred == 0 else \"This is a negative review.\"\n",
        "\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review1 = \"It made me cry.\"\n",
        "review2 = \"It was so good it made me cry.\"\n",
        "review3 = \"It's ok.\"\n",
        "review4 = \"I loved the dialogues!\"\n",
        "review5 = \"Garbage\"\n",
        "\n",
        "predict(model, review1)\n",
        "predict(model, review2)\n",
        "predict(model, review3)\n",
        "predict(model, review4)\n",
        "predict(model, review5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU4klLInBmnU",
        "outputId": "10579b41-71bc-4696-cb01-a430e9928821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a negative review.\n",
            "This is a negative review.\n",
            "This is a positive review.\n",
            "This is a positive review.\n",
            "This is a negative review.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks\n",
        "\n",
        "1. Initialize model embedding layer with pre-trained word2vec embeddings. Train new model with obtained layer. Compare results with  on test dataset.\n",
        "\n",
        "2. Optimize the training process by\n",
        " - Introducting early stopping\n",
        " - Experimenting on various ways of weight initialisation\n",
        " - Selecting hyperparameters with [Optuna](https://optuna.org/)"
      ],
      "metadata": {
        "id": "LFcezZaVaEZX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8eiiO1CX_Lq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}